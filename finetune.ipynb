{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPOzdMPzh1644FoMKgjAJ6m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-newhauser/rep-or-dem-tweets/blob/main/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYEzXUtssbda",
        "outputId": "e5891165-9282-4235-c81d-d06064d8b7de"
      },
      "source": [
        "!pip install transformers==4.6.0\n",
        "!pip install tweet-preprocessor\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.6.0 in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (0.0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (0.0.46)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (4.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (3.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.0) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0) (3.0.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (1.15.0)\n",
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP6Ij2iZsoaF"
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import preprocessor as p\n",
        "\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from transformers import (\n",
        "    TFDistilBertForSequenceClassification,\n",
        "    TFTrainer,\n",
        "    TFTrainingArguments,\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "random.seed(123)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06s5jd0GssDd"
      },
      "source": [
        "# Load data from fivethirtyeight\n",
        "tweets = pd.read_csv(\"senators_training.csv\")\n",
        "tweets_validation = pd.read_csv(\"senators_validation.csv\")\n",
        "\n",
        "# Remove numbers, emojis and &'s\n",
        "p.set_options(p.OPT.NUMBER, p.OPT.EMOJI)\n",
        "tweets[\"text_clean\"] = tweets[\"text\"].apply(p.clean)\n",
        "tweets[\"text_clean\"] = tweets[\"text_clean\"].str.replace(\"&amp;\", \"and \")\n",
        "tweets_validation[\"text_clean\"] = tweets_validation[\"text\"].apply(p.clean)\n",
        "tweets_validation[\"text_clean\"] = tweets_validation[\"text_clean\"].str.replace(\"&amp;\", \"and \")\n",
        "\n",
        "# Truncate tweets to max 512\n",
        "tweets[\"text_clean\"] = tweets[\"text_clean\"].str[:512]\n",
        "tweets_validation[\"text_clean\"] = tweets_validation[\"text_clean\"].str[:512]\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suYnas2CYm0O"
      },
      "source": [
        "# Create a column with numeric labels\n",
        "label_mapping = {\n",
        "    \"D\": 0,\n",
        "    \"R\": 1\n",
        "}\n",
        "\n",
        "tweets['label'] = np.where(tweets['party'] == \"D\", 0, 1)\n",
        "tweets_validation['label'] = np.where(tweets_validation['party'] == \"D\", 0, 1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTqSv2n6jPjc"
      },
      "source": [
        "# Convert to list\n",
        "texts = list(tweets.text)\n",
        "labels = list(tweets.label)\n",
        "val_texts = list(tweets_validation.text)\n",
        "val_labels = list(tweets_validation.label)\n",
        "\n",
        "# Split training dataset into test and train\n",
        "(train_texts, test_texts, train_labels, test_labels) = train_test_split(\n",
        "    texts, labels, test_size=0.3\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSfCp9LgTe2M",
        "outputId": "0184c57f-34be-46b0-d0e6-fd46a16d85a7"
      },
      "source": [
        "print(type(train_texts))\n",
        "print(type(train_labels))\n",
        "\n",
        "print(train_texts[0])\n",
        "print(train_labels[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Corker on @CNN: Bipartisan majority in Congress and vast majority of the American people oppose the #BadIranDeal: http://t.co/VaHTJ2RoGH.\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_MnqOr2tmuv"
      },
      "source": [
        "# Load DistilBERT tokenizer and tokenize (encode) the texts\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qot-5NA0VOgG",
        "outputId": "ac118574-d2a4-4658-ed88-42b949c08353"
      },
      "source": [
        "print(type(train_encodings))\n",
        "print(train_encodings[10])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "Encoding(num_tokens=63, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSCu1830s6Sx",
        "outputId": "c568e442-28d2-4498-8bb9-b057380ac746"
      },
      "source": [
        "# Wrap encodings in a Tensor Flow dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))\n",
        "\n",
        "# Learning rate, epochs, batch size\n",
        "\n",
        "# Set the training arguments\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir=\"./results\",  # output directory\n",
        "    num_train_epochs=6,  # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
        "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,  # strength of weight decay\n",
        "    logging_dir=\"./logs\",  # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "with training_args.strategy.scope():\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\", \n",
        "        num_labels=2\n",
        "    )\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model,  # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,  # training arguments, defined above\n",
        "    train_dataset=train_dataset,  # training dataset\n",
        "    eval_dataset=val_dataset,  # evaluation dataset\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_99', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weHFBpPys9RC",
        "outputId": "d81d38a6-face-4d61-b188-906a6c06709a"
      },
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcWCLXREt04T",
        "outputId": "3269642c-a8c8-4d1e-b394-8893b49888d2"
      },
      "source": [
        "# Evaluate the model\n",
        "trainer.evaluate()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.6836820125579834}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvGr9UwTlBn5",
        "outputId": "8f2d0228-1481-4cda-84bb-286232e1a6c2"
      },
      "source": [
        "# Get predictions\n",
        "predictions = trainer.predict(val_dataset)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "compute_metrics(predictions)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.65,\n",
              " 'f1': 0.6708463949843261,\n",
              " 'precision': 0.6331360946745562,\n",
              " 'recall': 0.7133333333333334}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RC10DhBqTub"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}